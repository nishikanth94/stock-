{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#import Tkinter as tk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import datetime as dt\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'isin', u'varchar(255)', u'YES', u'', None, u'')\n",
      "(u'insertion_datetime', u'datetime', u'YES', u'', None, u'')\n",
      "(u'datetime', u'datetime', u'YES', u'', None, u'')\n",
      "(u'open', u'float(10,2)', u'YES', u'', None, u'')\n",
      "(u'high', u'float(10,2)', u'YES', u'', None, u'')\n",
      "(u'low', u'float(10,2)', u'YES', u'', None, u'')\n",
      "(u'close', u'float(10,2)', u'YES', u'', None, u'')\n",
      "(u'volume', u'int(11)', u'YES', u'', None, u'')\n",
      "(u'open_interest', u'int(11)', u'YES', u'', None, u'')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import mysql.connector\n",
    "import numpy as np\n",
    "conn = mysql.connector.connect(\n",
    "         user='foouser',\n",
    "         password='F88Pa%%**',\n",
    "         host='134.209.144.239',\n",
    "         database='stocksdb')\n",
    "\n",
    "cur=conn.cursor()\n",
    "#print(conn)\n",
    "#cur.execute(\"select * from stocksdb.interview\")\n",
    "#cur.execute(\"SHOW COLUMNS FROM interview\")\n",
    "\n",
    "cur.execute(\"describe interview\")\n",
    "df = []\n",
    "for column in cur.fetchall():\n",
    "\tdf.append(column)\n",
    "\tprint(column)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/home/nishi/Desktop\")\n",
    "df=pd.read_csv(\"stock.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                isin   insertion_datetime             datetime     open  \\\n",
      "489906  INE492A01029                  NaN  2017-04-03 09:15:00   717.80   \n",
      "34875   INE545A01016                  NaN  2017-04-03 09:15:00   223.45   \n",
      "489907  INE492A01029                  NaN  2017-04-03 09:16:00   718.00   \n",
      "34876   INE545A01016                  NaN  2017-04-03 09:16:00   223.45   \n",
      "489908  INE492A01029                  NaN  2017-04-03 09:17:00   723.75   \n",
      "34877   INE545A01016                  NaN  2017-04-03 09:17:00   223.45   \n",
      "489909  INE492A01029                  NaN  2017-04-03 09:18:00   719.05   \n",
      "409374     dummy3751                  NaN  2017-04-03 09:18:00    20.25   \n",
      "34878   INE545A01016                  NaN  2017-04-03 09:18:00   223.45   \n",
      "306327  INE221B01012                  NaN  2017-04-03 09:19:00  2793.95   \n",
      "409375     dummy3751                  NaN  2017-04-03 09:19:00    20.25   \n",
      "34879   INE545A01016                  NaN  2017-04-03 09:19:00   223.45   \n",
      "489910  INE492A01029                  NaN  2017-04-03 09:19:00   720.00   \n",
      "112703  INE265F01028                  NaN  2017-04-03 09:19:00   829.60   \n",
      "34880   INE545A01016                  NaN  2017-04-03 09:20:00   223.45   \n",
      "409376     dummy3751                  NaN  2017-04-03 09:20:00    20.20   \n",
      "489911  INE492A01029                  NaN  2017-04-03 09:20:00   719.00   \n",
      "112704  INE265F01028                  NaN  2017-04-03 09:20:00   831.00   \n",
      "306328  INE221B01012                  NaN  2017-04-03 09:20:00  2793.95   \n",
      "306329  INE221B01012                  NaN  2017-04-03 09:21:00  2793.95   \n",
      "489912  INE492A01029                  NaN  2017-04-03 09:21:00   718.00   \n",
      "409377     dummy3751                  NaN  2017-04-03 09:21:00    20.20   \n",
      "34881   INE545A01016                  NaN  2017-04-03 09:21:00   223.45   \n",
      "112705  INE265F01028                  NaN  2017-04-03 09:21:00   836.00   \n",
      "112706  INE265F01028                  NaN  2017-04-03 09:22:00   836.00   \n",
      "34882   INE545A01016                  NaN  2017-04-03 09:22:00   223.45   \n",
      "489913  INE492A01029                  NaN  2017-04-03 09:22:00   719.95   \n",
      "306330  INE221B01012                  NaN  2017-04-03 09:22:00  2766.05   \n",
      "409378     dummy3751                  NaN  2017-04-03 09:22:00    20.25   \n",
      "112707  INE265F01028                  NaN  2017-04-03 09:23:00   836.00   \n",
      "...              ...                  ...                  ...      ...   \n",
      "271978  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:00:00  3724.05   \n",
      "271979  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:01:00  3723.00   \n",
      "271980  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:02:00  3719.95   \n",
      "271981  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:03:00  3719.00   \n",
      "271982  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:04:00  3718.50   \n",
      "271983  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:05:00  3717.10   \n",
      "271984  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:06:00  3718.60   \n",
      "271985  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:07:00  3716.90   \n",
      "271986  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:08:00  3716.00   \n",
      "271987  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:09:00  3717.00   \n",
      "271988  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:10:00  3715.00   \n",
      "271989  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:11:00  3714.00   \n",
      "271990  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:12:00  3703.00   \n",
      "271991  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:13:00  3703.40   \n",
      "271992  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:14:00  3704.80   \n",
      "271993  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:15:00  3705.00   \n",
      "271994  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:16:00  3699.80   \n",
      "271995  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:17:00  3697.15   \n",
      "271996  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:18:00  3700.00   \n",
      "271997  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:19:00  3702.00   \n",
      "271998  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:20:00  3709.80   \n",
      "271999  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:21:00  3705.40   \n",
      "272000  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:22:00  3710.00   \n",
      "272001  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:23:00  3703.70   \n",
      "272002  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:24:00  3704.65   \n",
      "272003  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:25:00  3700.00   \n",
      "272004  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:26:00  3704.00   \n",
      "272005  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:27:00  3707.00   \n",
      "272006  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:28:00  3704.90   \n",
      "272007  INE545A01016  2019-01-23 17:10:27  2018-12-31 15:29:00  3706.00   \n",
      "\n",
      "           high      low    close  volume  open_interest  \n",
      "489906   718.00   717.80   718.00      27            NaN  \n",
      "34875    223.45   223.45   223.45      20            NaN  \n",
      "489907   723.95   718.00   723.75      61            NaN  \n",
      "34876    223.45   223.45   223.45       0            NaN  \n",
      "489908   723.75   719.05   719.05     100            NaN  \n",
      "34877    223.45   223.45   223.45       0            NaN  \n",
      "489909   719.05   719.05   719.05       0            NaN  \n",
      "409374    20.25    20.25    20.25    6000            NaN  \n",
      "34878    223.45   223.45   223.45       0            NaN  \n",
      "306327  2793.95  2793.95  2793.95      10            NaN  \n",
      "409375    20.25    20.20    20.20    6000            NaN  \n",
      "34879    223.45   223.45   223.45     100            NaN  \n",
      "489910   720.00   720.00   720.00      10            NaN  \n",
      "112703   834.70   829.00   831.00      39            NaN  \n",
      "34880    223.45   223.45   223.45       0            NaN  \n",
      "409376    20.20    20.20    20.20       0            NaN  \n",
      "489911   719.00   718.00   718.05      80            NaN  \n",
      "112704   831.00   831.00   831.00       0            NaN  \n",
      "306328  2793.95  2793.95  2793.95       0            NaN  \n",
      "306329  2793.95  2766.05  2766.05       7            NaN  \n",
      "489912   718.00   718.00   718.00     100            NaN  \n",
      "409377    20.25    20.20    20.25    6000            NaN  \n",
      "34881    223.45   223.45   223.45       0            NaN  \n",
      "112705   836.00   836.00   836.00     100            NaN  \n",
      "112706   836.00   836.00   836.00       0            NaN  \n",
      "34882    223.45   223.45   223.45       0            NaN  \n",
      "489913   719.95   716.45   716.50     181            NaN  \n",
      "306330  2781.00  2766.05  2781.00       5            NaN  \n",
      "409378    20.25    20.25    20.25       0            NaN  \n",
      "112707   836.00   836.00   836.00       0            NaN  \n",
      "...         ...      ...      ...     ...            ...  \n",
      "271978  3725.00  3721.60  3723.00     437          437.0  \n",
      "271979  3723.50  3718.00  3719.95    1488         1488.0  \n",
      "271980  3719.95  3717.00  3719.00     630          630.0  \n",
      "271981  3719.00  3716.00  3718.50     197          197.0  \n",
      "271982  3721.20  3717.00  3717.10     548          548.0  \n",
      "271983  3718.75  3717.10  3718.60     324          324.0  \n",
      "271984  3718.60  3715.10  3716.95     336          336.0  \n",
      "271985  3717.00  3715.10  3717.00     438          438.0  \n",
      "271986  3718.00  3715.10  3718.00     599          599.0  \n",
      "271987  3718.00  3715.00  3715.00     955          955.0  \n",
      "271988  3717.00  3713.00  3714.00     812          812.0  \n",
      "271989  3714.70  3702.25  3702.25    2488         2488.0  \n",
      "271990  3705.00  3702.30  3703.40    1405         1405.0  \n",
      "271991  3704.95  3703.40  3704.95     397          397.0  \n",
      "271992  3705.00  3704.80  3704.95     895          895.0  \n",
      "271993  3705.00  3696.50  3699.80    2954         2954.0  \n",
      "271994  3699.80  3696.65  3697.10     932          932.0  \n",
      "271995  3700.00  3696.00  3700.00    1001         1001.0  \n",
      "271996  3705.00  3699.10  3704.40    1107         1107.0  \n",
      "271997  3710.00  3702.00  3709.80    1305         1305.0  \n",
      "271998  3709.95  3705.00  3707.00    1068         1068.0  \n",
      "271999  3709.80  3705.40  3707.85     498          498.0  \n",
      "272000  3710.00  3703.70  3703.70     590          590.0  \n",
      "272001  3705.05  3702.05  3704.65     722          722.0  \n",
      "272002  3704.65  3700.00  3701.40     686          686.0  \n",
      "272003  3705.00  3700.00  3703.10    1334         1334.0  \n",
      "272004  3709.95  3703.05  3707.00     816          816.0  \n",
      "272005  3709.95  3703.10  3703.10    1192         1192.0  \n",
      "272006  3705.00  3700.20  3702.00     868          868.0  \n",
      "272007  3707.50  3702.00  3707.00    1431         1431.0  \n",
      "\n",
      "[500000 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nf=df.sort_values('datetime')\n",
    "print(nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "high_prices= nf.iloc[:,[4]].as_matrix()\n",
    "low_prices = nf.iloc[:,[5]].as_matrix()\n",
    "#print(\"high_prices\",high_prices,high_prices.shape)\n",
    "#print(\"low_prices\",low_prices)\n",
    "m_p = []\n",
    "for i in range(high_prices.shape[0]):\n",
    "\tm_p.append((float(high_prices[i])+float(low_prices[i]))/2.0)\n",
    "\t#print(m_p)\n",
    "#print(high_prices[i],\"-----\",low_prices[i],\"-----\",m_p)\n",
    "\n",
    "mid_prices=np.array(m_p)\n",
    "#print(\"mid_prices\",mid_prices)\n",
    "#print(\"shape\",mid_prices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train data', array([ 717.9  ,  223.45 ,  720.975, ...,   10.1  , 3268.625,  710.05 ]))\n"
     ]
    }
   ],
   "source": [
    "train_data = mid_prices[:350000]\n",
    "test_data = mid_prices[150000:]\n",
    "print(\"train data\", train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('test_data', array([-0.00253826, -0.00239747, -0.00252461, ...,  1.06551543,\n",
      "        1.06438436,  1.06500393]))\n"
     ]
    }
   ],
   "source": [
    "#scaling using min-max scalar\n",
    "scaler = MinMaxScaler()\n",
    "train_data = train_data.reshape(-1,1)\n",
    "test_data = test_data.reshape(-1,1)\n",
    "\n",
    "smoothing_window_size = 2500\n",
    "for di in range(0,10000,smoothing_window_size):\n",
    "    scaler.fit(train_data[di:di+smoothing_window_size,:])\n",
    "    train_data[di:di+smoothing_window_size,:] = scaler.transform(train_data[di:di+smoothing_window_size,:])\n",
    "\n",
    "#normalize the last bit of remaining data\n",
    "scaler.fit(train_data[di+smoothing_window_size:,:])\n",
    "train_data[di+smoothing_window_size:,:] = scaler.transform(train_data[di+smoothing_window_size:,:])\n",
    "\n",
    "# Reshape both train and test data\n",
    "train_data = train_data.reshape(-1)\n",
    "\n",
    "# Normalize test data\n",
    "test_data = scaler.transform(test_data).reshape(-1)\n",
    "print(\"test_data\",test_data)\n",
    "\n",
    "EMA = 0.0\n",
    "gamma = 0.1\n",
    "for ti in range(11000):\n",
    "  EMA = gamma*train_data[ti] + (1-gamma)*EMA\n",
    "  train_data[ti] = EMA\n",
    "\n",
    "# Used for visualization and test purposes\n",
    "all_mid_data = np.concatenate([train_data,test_data],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE error for standard averaging: 0.03121\n"
     ]
    }
   ],
   "source": [
    "#average\n",
    "window_size = 100\n",
    "N = train_data.size\n",
    "std_avg_predictions = []\n",
    "std_avg_x = []\n",
    "mse_errors = []\n",
    "\n",
    "for pred_idx in range(window_size,N):\n",
    "\n",
    "    if pred_idx >= N:\n",
    "        date = dt.datetime.strptime(k, '%Y-%m-%d').date() + dt.timedelta(days=1)\n",
    "    else:\n",
    "        date = df.loc[pred_idx,'datetime']\n",
    "\n",
    "    std_avg_predictions.append(np.mean(train_data[pred_idx-window_size:pred_idx]))\n",
    "    mse_errors.append((std_avg_predictions[-1]-train_data[pred_idx])**2)\n",
    "    std_avg_x.append(date)\n",
    "\n",
    "print('MSE error for standard averaging: %.5f'%(0.5*np.mean(mse_errors)))\n",
    "\n",
    "plt.figure(figsize = (18,9))\n",
    "plt.plot(all_mid_data,color='b',label='True')\n",
    "plt.plot(std_avg_predictions,color='orange',label='Prediction')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mid Price')\n",
    "plt.legend(fontsize=18)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE error for EMA averaging: 0.04810\n"
     ]
    }
   ],
   "source": [
    "#exponential average\n",
    "window_size = 100\n",
    "N = train_data.size\n",
    "\n",
    "run_avg_predictions = []\n",
    "run_avg_x = []\n",
    "\n",
    "mse_errors = []\n",
    "\n",
    "running_mean = 0.0\n",
    "run_avg_predictions.append(running_mean)\n",
    "\n",
    "decay = 0.5\n",
    "\n",
    "for pred_idx in range(1,N):\n",
    "\n",
    "    running_mean = running_mean*decay + (1.0-decay)*train_data[pred_idx-1]\n",
    "    run_avg_predictions.append(running_mean)\n",
    "    mse_errors.append((run_avg_predictions[-1]-train_data[pred_idx])**2)\n",
    "    run_avg_x.append(date)\n",
    "\n",
    "print('MSE error for EMA averaging: %.5f'%(0.5*np.mean(mse_errors)))\n",
    "\n",
    "plt.figure(figsize = (18,9))\n",
    "plt.plot(all_mid_data,color='r',label='True')\n",
    "plt.plot(run_avg_predictions,color='orange', label='Prediction')\n",
    "#plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mid Price')\n",
    "plt.legend(fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Data augmentation\n",
    "class DataGeneratorSeq(object):\n",
    "\n",
    "    def __init__(self,prices,batch_size,num_unroll):\n",
    "        self._prices = prices\n",
    "        self._prices_length = len(self._prices) - num_unroll\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unroll = num_unroll\n",
    "        self._segments = self._prices_length //self._batch_size\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "\n",
    "    def next_batch(self):\n",
    "\n",
    "        batch_data = np.zeros((self._batch_size),dtype=np.float32)\n",
    "        batch_labels = np.zeros((self._batch_size),dtype=np.float32)\n",
    "\n",
    "        for b in range(self._batch_size):\n",
    "            if self._cursor[b]+1>=self._prices_length:\n",
    "                #self._cursor[b] = b * self._segments\n",
    "                self._cursor[b] = np.random.randint(0,(b+1)*self._segments)\n",
    "\n",
    "            batch_data[b] = self._prices[self._cursor[b]]\n",
    "            batch_labels[b]= self._prices[self._cursor[b]+np.random.randint(0,5)]\n",
    "\n",
    "            self._cursor[b] = (self._cursor[b]+1)%self._prices_length\n",
    "\n",
    "        return batch_data,batch_labels\n",
    "\n",
    "    def unroll_batches(self):\n",
    "\n",
    "        unroll_data,unroll_labels = [],[]\n",
    "        init_data, init_label = None,None\n",
    "        for ui in range(self._num_unroll):\n",
    "\n",
    "            data, labels = self.next_batch()    \n",
    "\n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "\n",
    "        return unroll_data, unroll_labels\n",
    "\n",
    "    def reset_indices(self):\n",
    "        for b in range(self._batch_size):\n",
    "            self._cursor[b] = np.random.randint(0,min((b+1)*self._segments,self._prices_length-1))\n",
    "\n",
    "\n",
    "dg = DataGeneratorSeq(train_data,5,5)\n",
    "u_data, u_labels = dg.unroll_batches()\n",
    "\n",
    "for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):   \n",
    "    #print('\\n\\nUnrolled index %d'%ui)\n",
    "    dat_ind = dat\n",
    "    lbl_ind = lbl\n",
    "    #print('\\tInputs: ',dat )\n",
    "    #print('\\n\\tOutput:',lbl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "\n",
    "D = 1 # Dimensionality of the data. Since your data is 1-D this would be 1\n",
    "num_unrollings = 50 # Number of time steps you look into the future.\n",
    "batch_size = 500 # Number of samples in a batch\n",
    "num_nodes = [200,200,150] # Number of hidden nodes in each layer of the deep LSTM stack we're using\n",
    "n_layers = len(num_nodes) # number of layers\n",
    "dropout = 0.2 # dropout amount\n",
    "\n",
    "tf.reset_default_graph() # This is important in case you run this multiple times\n",
    "\n",
    "#defining input and output\n",
    "# Input data.\n",
    "train_inputs, train_outputs = [],[]\n",
    "\n",
    "# You unroll the input over time defining placeholders for each time step\n",
    "for ui in range(num_unrollings):\n",
    "    train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size,D],name='train_inputs_%d'%ui))\n",
    "    train_outputs.append(tf.placeholder(tf.float32, shape=[batch_size,1], name = 'train_outputs_%d'%ui))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-16-f6bec338b573>:7: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-16-f6bec338b573>:12: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#Defining Parameters of the LSTM and Regression layer\n",
    "lstm_cells = [\n",
    "    tf.contrib.rnn.LSTMCell(num_units=num_nodes[li],\n",
    "                            state_is_tuple=True,\n",
    "                            initializer= tf.contrib.layers.xavier_initializer()\n",
    "                           )\n",
    " for li in range(n_layers)]\n",
    "\n",
    "drop_lstm_cells = [tf.contrib.rnn.DropoutWrapper(\n",
    "    lstm, input_keep_prob=1.0,output_keep_prob=1.0-dropout, state_keep_prob=1.0-dropout\n",
    ") for lstm in lstm_cells]\n",
    "drop_multi_cell = tf.contrib.rnn.MultiRNNCell(drop_lstm_cells)\n",
    "multi_cell = tf.contrib.rnn.MultiRNNCell(lstm_cells)\n",
    "\n",
    "w = tf.get_variable('w',shape=[num_nodes[-1], 1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.get_variable('b',initializer=tf.random_uniform([1],-0.1,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-fb80f3ec755a>:18: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "#Calculating LSTM output and Feeding it to the regression layer to get final prediction\n",
    "# Create cell state and hidden state variables to maintain the state of the LSTM\n",
    "\n",
    "c, h = [],[]\n",
    "initial_state = []\n",
    "for li in range(n_layers):\n",
    "  c.append(tf.Variable(tf.zeros([batch_size, num_nodes[li]]), trainable=False))\n",
    "  h.append(tf.Variable(tf.zeros([batch_size, num_nodes[li]]), trainable=False))\n",
    "  initial_state.append(tf.contrib.rnn.LSTMStateTuple(c[li], h[li]))\n",
    "\n",
    "# Do several tensor transofmations, because the function dynamic_rnn requires the output to be of\n",
    "# a specific format. Read more at: https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\n",
    "all_inputs = tf.concat([tf.expand_dims(t,0) for t in train_inputs],axis=0)\n",
    "\n",
    "# all_outputs is [seq_length, batch_size, num_nodes]\n",
    "all_lstm_outputs, state = tf.nn.dynamic_rnn(\n",
    "    drop_multi_cell, all_inputs, initial_state=tuple(initial_state),\n",
    "    time_major = True, dtype=tf.float32)\n",
    "\n",
    "all_lstm_outputs = tf.reshape(all_lstm_outputs, [batch_size*num_unrollings,num_nodes[-1]])\n",
    "\n",
    "all_outputs = tf.nn.xw_plus_b(all_lstm_outputs,w,b)\n",
    "\n",
    "split_outputs = tf.split(all_outputs,num_unrollings,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining training Loss\n",
      "Learning rate decay operations\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Loss Calculation and Optimizer\n",
    "\n",
    "# When calculating the loss you need to be careful about the exact form, because you calculate\n",
    "# loss of all the unrolled steps at the same time\n",
    "# Therefore, take the mean error or each batch and get the sum of that over all the unrolled steps\n",
    "\n",
    "print('Defining training Loss')\n",
    "loss = 0.0\n",
    "with tf.control_dependencies([tf.assign(c[li], state[li][0]) for li in range(n_layers)]+\n",
    "                             [tf.assign(h[li], state[li][1]) for li in range(n_layers)]):\n",
    "  for ui in range(num_unrollings):\n",
    "    loss += tf.reduce_mean(0.5*(split_outputs[ui]-train_outputs[ui])**2)\n",
    "\n",
    "print('Learning rate decay operations')\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "inc_gstep = tf.assign(global_step,global_step + 1)\n",
    "tf_learning_rate = tf.placeholder(shape=None,dtype=tf.float32)\n",
    "tf_min_learning_rate = tf.placeholder(shape=None,dtype=tf.float32)\n",
    "\n",
    "learning_rate = tf.maximum(\n",
    "    tf.train.exponential_decay(tf_learning_rate, global_step, decay_steps=1, decay_rate=0.5, staircase=True),\n",
    "    tf_min_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Optimization operations\n",
      "\tAll done\n",
      "Defining prediction related TF functions\n"
     ]
    }
   ],
   "source": [
    "# Optimizer.\n",
    "print('TF Optimization operations')\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v))\n",
    "\n",
    "print('\\tAll done')\n",
    "\n",
    "print('Defining prediction related TF functions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_c, sample_h, initial_sample_state = [],[],[]\n",
    "for li in range(n_layers):\n",
    "  sample_c.append(tf.Variable(tf.zeros([1, num_nodes[li]]), trainable=False))\n",
    "  sample_h.append(tf.Variable(tf.zeros([1, num_nodes[li]]), trainable=False))\n",
    "  initial_sample_state.append(tf.contrib.rnn.LSTMStateTuple(sample_c[li],sample_h[li]))\n",
    "\n",
    "reset_sample_states = tf.group(*[tf.assign(sample_c[li],tf.zeros([1, num_nodes[li]])) for li in range(n_layers)],\n",
    "                               *[tf.assign(sample_h[li],tf.zeros([1, num_nodes[li]])) for li in range(n_layers)])\n",
    "\n",
    "\n",
    "sample_outputs, sample_state = tf.nn.dynamic_rnn(multi_cell, tf.expand_dims(sample_inputs,0),\n",
    "                                   initial_state=tuple(initial_sample_state),\n",
    "                                   time_major = True,\n",
    "                                   dtype=tf.float32)\n",
    "\n",
    "with tf.control_dependencies([tf.assign(sample_c[li],sample_state[li][0]) for li in range(n_layers)]+\n",
    "                              [tf.assign(sample_h[li],sample_state[li][1]) for li in range(n_layers)]):  \n",
    "  sample_prediction = tf.nn.xw_plus_b(tf.reshape(sample_outputs,[1,-1]), w, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# running low-short term memory\n",
    "\n",
    "epochs = 1\n",
    "valid_summary = 1 # Interval you make test predictions\n",
    "\n",
    "n_predict_once = 50 # Number of steps you continously predict for\n",
    "\n",
    "train_seq_length = train_data.size # Full length of the training data\n",
    "\n",
    "train_mse_ot = [] # Accumulate Train losses\n",
    "test_mse_ot = [] # Accumulate Test loss\n",
    "predictions_over_time = [] # Accumulate predictions\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Used for decaying learning rate\n",
    "loss_nondecrease_count = 0\n",
    "loss_nondecrease_threshold = 2 # If the test error hasn't increased in this many steps, decrease learning rate\n",
    "\n",
    "#print('Initialized')\n",
    "average_loss = 0\n",
    "\n",
    "# Define data generator\n",
    "data_gen = DataGeneratorSeq(train_data,batch_size,num_unrollings)\n",
    "\n",
    "x_axis_seq = []\n",
    "\n",
    "# Points you start your test predictions from\n",
    "test_points_seq = np.arange(11000,12000,50).tolist()\n",
    "\n",
    "for ep in range(epochs):       \n",
    "\n",
    "    # ========================= Training =====================================\n",
    "    for step in range(train_seq_length//batch_size):\n",
    "\n",
    "        u_data, u_labels = data_gen.unroll_batches()\n",
    "\n",
    "        feed_dict = {}\n",
    "        for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):            \n",
    "            feed_dict[train_inputs[ui]] = dat.reshape(-1,1)\n",
    "            feed_dict[train_outputs[ui]] = lbl.reshape(-1,1)\n",
    "\n",
    "        feed_dict.update({tf_learning_rate: 0.0001, tf_min_learning_rate:0.000001})\n",
    "\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "\n",
    "        average_loss += l\n",
    "\n",
    "    # ============================ Validation ==============================\n",
    "    if (ep+1) % valid_summary == 0:\n",
    "\n",
    "      average_loss = average_loss/(valid_summary*(train_seq_length//batch_size))\n",
    "\n",
    "      # The average loss\n",
    "      if (ep+1)%valid_summary==0:\n",
    "        print('Average loss at step %d: %f' % (ep+1, average_loss))\n",
    "\n",
    "      train_mse_ot.append(average_loss)\n",
    "\n",
    "      average_loss = 0 # reset loss\n",
    "\n",
    "      predictions_seq = []\n",
    "\n",
    "      mse_test_loss_seq = []\n",
    "\n",
    "      # ===================== Updating State and Making Predicitons ========================\n",
    "      for w_i in test_points_seq:\n",
    "        mse_test_loss = 0.0\n",
    "        our_predictions = []\n",
    "\n",
    "        if (ep+1)-valid_summary==0:\n",
    "          # Only calculate x_axis values in the first validation epoch\n",
    "          x_axis=[]\n",
    "\n",
    "        # Feed in the recent past behavior of stock prices\n",
    "        # to make predictions from that point onwards\n",
    "        for tr_i in range(w_i-num_unrollings+1,w_i-1):\n",
    "          current_price = all_mid_data[tr_i]\n",
    "          feed_dict[sample_inputs] = np.array(current_price).reshape(1,1)    \n",
    "          _ = session.run(sample_prediction,feed_dict=feed_dict)\n",
    "\n",
    "        feed_dict = {}\n",
    "\n",
    "        current_price = all_mid_data[w_i-1]\n",
    "\n",
    "        feed_dict[sample_inputs] = np.array(current_price).reshape(1,1)\n",
    "\n",
    "        # Make predictions for this many steps\n",
    "        # Each prediction uses previous prediciton as it's current input\n",
    "        for pred_i in range(n_predict_once):\n",
    "\n",
    "          pred = session.run(sample_prediction,feed_dict=feed_dict)\n",
    "\n",
    "          our_predictions.append(np.asscalar(pred))\n",
    "\n",
    "          feed_dict[sample_inputs] = np.asarray(pred).reshape(-1,1)\n",
    "\n",
    "          if (ep+1)-valid_summary==0:\n",
    "            # Only calculate x_axis values in the first validation epoch\n",
    "            x_axis.append(w_i+pred_i)\n",
    "\n",
    "          mse_test_loss += 0.5*(pred-all_mid_data[w_i+pred_i])**2\n",
    "\n",
    "        session.run(reset_sample_states)\n",
    "\n",
    "        predictions_seq.append(np.array(our_predictions))\n",
    "\n",
    "        mse_test_loss /= n_predict_once\n",
    "        mse_test_loss_seq.append(mse_test_loss)\n",
    "\n",
    "        if (ep+1)-valid_summary==0:\n",
    "          x_axis_seq.append(x_axis)\n",
    "\n",
    "      current_test_mse = np.mean(mse_test_loss_seq)\n",
    "\n",
    "      # Learning rate decay logic\n",
    "      if len(test_mse_ot)>0 and current_test_mse > min(test_mse_ot):\n",
    "          loss_nondecrease_count += 1\n",
    "      else:\n",
    "          loss_nondecrease_count = 0\n",
    "\n",
    "      if loss_nondecrease_count > loss_nondecrease_threshold :\n",
    "            session.run(inc_gstep)\n",
    "            loss_nondecrease_count = 0\n",
    "            print('\\tDecreasing learning rate by 0.5')\n",
    "\n",
    "      test_mse_ot.append(current_test_mse)\n",
    "      print('\\tTest MSE: %.5f'%np.mean(mse_test_loss_seq))\n",
    "      predictions_over_time.append(predictions_seq)\n",
    "      print('\\tFinished Predictions')\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
